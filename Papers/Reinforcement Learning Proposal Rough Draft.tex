%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Journal Article
% LaTeX Template
% Version 1.4 (15/5/16)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com) with extensive modifications by
% Vel (vel@LaTeXTemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[twoside,twocolumn]{article}

\usepackage{blindtext} % Package to generate dummy text throughout this template 

\usepackage[sc]{mathpazo} % Use the Palatino font
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\linespread{1.05} % Line spacing - Palatino needs more space between lines
\usepackage{microtype} % Slightly tweak font spacing for aesthetics

\usepackage{amsmath}

\usepackage[english]{babel} % Language hyphenation and typographical rules

\usepackage[hmarginratio=1:1,top=32mm,columnsep=20pt]{geometry} % Document margins
\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption} % Custom captions under/above floats in tables or figures
\usepackage{booktabs} % Horizontal rules in tables

\usepackage{lettrine} % The lettrine is the first enlarged letter at the beginning of the text

\usepackage{xcolor}

\usepackage{enumitem} % Customized lists
\setlist[itemize]{noitemsep} % Make itemize lists more compact

\usepackage{abstract} % Allows abstract customization
\renewcommand{\abstractnamefont}{\normalfont\bfseries} % Set the "Abstract" text to bold
\renewcommand{\abstracttextfont}{\normalfont\small\itshape} % Set the abstract itself to small italic text

\usepackage{titlesec} % Allows customization of titles
\renewcommand\thesection{\Roman{section}} % Roman numerals for the sections
\renewcommand\thesubsection{\roman{subsection}} % roman numerals for subsections
\titleformat{\section}[block]{\large\scshape\centering}{\thesection.}{1em}{} % Change the look of the section titles
\titleformat{\subsection}[block]{\large}{\thesubsection.}{1em}{} % Change the look of the section titles

\usepackage{fancyhdr} % Headers and footers
\pagestyle{fancy} % All pages have headers and footers
\fancyhead{} % Blank out the default header
\fancyfoot{} % Blank out the default footer
\fancyhead[C]{Reinforcement Learning  $\bullet$ September 2021 }% Custom header text
\fancyfoot[RO,LE]{\thepage} % Custom footer text

\usepackage{titling} % Customizing the title section

\usepackage{hyperref} % For hyperlinks in the PDF

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\setlength{\droptitle}{-4\baselineskip} % Move the title up

\pretitle{\begin{center}\Huge\bfseries} % Article title formatting
\posttitle{\end{center}} % Article title closing formatting
\title{Machine Learning: Reinforced Learning} % Article title
\author{%
\textsc{Robert Horton}\thanks{"Denim"} \\[1ex] % Your name
\normalsize University of Colorado Springs Colorado \\ % Your institution
\normalsize \href{mailto:rhorton2@uccs.edu}{rhorton2@uccs.edu} % Your email address
\and % Uncomment if 2 authors are required, duplicate these 4 lines if more
\textsc{Gaberial Yeager}\thanks{"Gabe"} \\[1ex] % Second author's name
\normalsize University of Colorado Springs Colorado \\ % Your institution
\normalsize \href{mailto:gyeager@uccs.edu}{gyeager@uccs.edu} % Second author's email address
}
\date{\today} % Leave empty to omit a date
\renewcommand{\maketitlehookd}{%
\begin{abstract}
 \noindent ABSTRACT HERE!!!!!

\end{abstract}
}

\renewcommand \thesection{\arabic{section}}
\renewcommand \thesubsection{\arabic{section}.\arabic{subsection}}
%----------------------------------------------------------------------------------------

\begin{document}

% Print the title
\maketitle

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section{Introduction}

\lettrine[nindent=0em,lines=3]{B}lackjack is a commonly known game that can be played with different types of methods of approach.

%------------------------------------------------

\section{Background}

Reinforcement learning, although considered a part of machine learning, is very different from the other types of machine learning.  Two main categories that the different types of machine learning fall under are supervised learning\footnote{Explanation of supervised learning} and unsupervised learning \footnote{Explanation of unsupervised learning} \cite{sutton:2018}.  Although supervised learning can be used to speed along the process of machine learning this is not always an option when deploying a machine learning process into an unexplored environment.  These types of implementations call upon reinforcement learning to perform an unsupervised learning process.  Now when performing these unsupervised approaches of machine learning there are many different methods that have been used and studied over the years.  Finding out which method is the most efficient and works best for training a machine to learn how to play blackjack will be the topic of this paper.\\ 

\indent When considering which method of RL\footnote{Reinforcement Learning} to use when wanting to teach a machine how to play the game blackjack, there are many different options to consider.  When considering the game blackjack the next cards that will be drawn are always unknown to the players and dealers.  The hole card\footnote{Face down card that is dealt to dealer at the beginning of each game} is also a card that is not known to any player including the dealer, making this game an even more inherently stochastic model.  Since bets are placed while playing this game, maximizing the profit on return for each game played is very important and is hard to do with the uncertainty of what the next card dealt will be and what hole card has been dealt to the dealer.  This models the perfect Markov Decision Process where each game promises states and actions that are unknown and have not been explored yet.\\

\indent When considering reinforcement learning it can be broken down further into two categories “On Policy” or “Off Policy” learning.  On policy RL approaches are considered as methods that involve learning agents that learn from the value function denoted as $v_{\pi}$ or $q_{\pi}$ according to the current occupied state which can also be denoted as $v_{\pi}(s)$  or $q_{\pi}(s)$ for $s \in S$ where $S$ is the set of all possible states.   Off policy works when the learning agents learn from the value function of the previous occupied state \cite{GeeksforGeeks:2021dg}.  Above is a table that will be referred to when discussing what methods will be used for approaching these problems. \\ 

\begin{table}
\caption{Example table}
\centering
\begin{tabular}{llr}
\toprule
\multicolumn{2}{c}{Blackjack RL Methods} \\
\cmidrule(r){1-2}
Name & On/Off-Policy  & Greedy \\
\midrule
Q Learning & Off-Policy & True \\
SARSA & On-Policy & True \\
T{\tiny emperoal} D{\tiny iffernece} & Off-Policy & False \\
\bottomrule
\end{tabular}
\end{table}

%------------------------------------------------

\subsection{Q-Learning}

\indent One of the most well known and oldest methods listed is the “Q-Learning” approach.  This approach is considered an off-policy TD control algorithm that can be defined as\\

\begin{align*}
 Q(S_t,A_t) \leftarrow & Q(S_t,A_t)+ \alpha\big[ R_{t+1} +\\
& \gamma \binom{max}{a} Q(S_{t+1}, a) - Q(S_t,A_t)\big]
\end{align*}

[1]\\
, where the steps taken are independent of what the next updated policy might be based upon that action.  Depending upon the value of $\epsilon$ the frequency of random steps taken can vary and make the model even more stochastic.  What is important to note is that although it might be able to ensure a steady increase in profit after each game faster a not so greedy approach could in the long run ensure more consistent and bigger profits over a longer period of multiple games. With each action taken the next one could be random. The two actions are independent in the sense that the previous step was not taken based on the reward given from the action performed to move into now occupied state. 

%------------------------------------------------

\subsection{SARSA}

\indent The “SARSA” \footnote{State Action Reward State Action} is a slightly modified version of the Q -learning approach where the modification of the policy differs. Q-learning is considered an off policy where SARSA is an on policy \cite{GeeksforGeeks:2021dg}. 

%------------------------------------------------

\subsection{Temporal Difference}

The third and last method that will be considered for application in this problem is “Temporal Difference“ suggested by Dr. Andrew G. Barto \cite{sutton:2018}.  Temporal difference involves adjusting the policy based on a estimation of the sum of future actions to be made.  This method, unlike the rest, takes a greedy approach that will form decisions on what actions to take next based off of the newly calculated policy from known possible actions in states.  All of these are important and vary in the approaches for adjusting and reacting to the policy that helps are self-learning machines decide what moves to make next and at what rate they learn new things and ultimately uncover the best policy to solving a problem. 

%------------------------------------------------

\section{Related Work}

%------------------------------------------------

\section{Methodology}

A statement requiring citation \cite{Figueredo:2009dg}.

%------------------------------------------------

\section{Conclusion}

\pagebreak
%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------

\bibliography{Reinforcement Learning Proposal Rough Draft.bib}

\begin{thebibliography}{99} % Bibliography - this is intentionally simple in this template
\bibitem[Sutton, R. S., \& Barto, A. G. , 2018]{sutton:2018}
Sutton, R. S., \& Barto, A. G. (2018).
\newblock Reinforcement Learning: An Introduction. Cambridge (Mass.): 
\newblock {\em The MIT Press.}, .

\bibitem[GeeksforGeeks]{GeeksforGeeks:2021dg}
“Sarsa Reinforcement Learning.” {\em GeeksforGeeks}, \\
24 June 2021,\\
\newblock www.geeksforgeeks.org/sarsa-reinforcement-learning/. 


\bibitem[Figueredo and Wolf, 2009]{Figueredo:2009dg}
Figueredo, A.~J. and Wolf, P. S.~A. (2009).
\newblock Assortative pairing and life history strategy - a cross-cultural
  study.
\newblock {\em Human Nature}, 20:317--330.
\end{thebibliography}

%----------------------------------------------------------------------------------------

\end{document}
