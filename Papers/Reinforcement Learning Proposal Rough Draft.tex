%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Journal Article
% LaTeX Template
% Version 1.4 (15/5/16)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Frits Wenneker (http://www.howtotex.com) with extensive modifications by
% Vel (vel@LaTeXTemplates.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[twoside,twocolumn]{article}

\usepackage{blindtext} % Package to generate dummy text throughout this template 

\usepackage[sc]{mathpazo} % Use the Palatino font
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\linespread{1.05} % Line spacing - Palatino needs more space between lines
\usepackage{microtype} % Slightly tweak font spacing for aesthetics

\usepackage{amsmath}

\usepackage[english]{babel} % Language hyphenation and typographical rules

\usepackage[hmarginratio=1:1,top=32mm,columnsep=20pt]{geometry} % Document margins
\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption} % Custom captions under/above floats in tables or figures
\usepackage{booktabs} % Horizontal rules in tables

\usepackage{lettrine} % The lettrine is the first enlarged letter at the beginning of the text

\usepackage{xcolor}

\usepackage{enumitem} % Customized lists
\setlist[itemize]{noitemsep} % Make itemize lists more compact

\usepackage{abstract} % Allows abstract customization
\renewcommand{\abstractnamefont}{\normalfont\bfseries} % Set the "Abstract" text to bold
\renewcommand{\abstracttextfont}{\normalfont\small\itshape} % Set the abstract itself to small italic text

\usepackage{titlesec} % Allows customization of titles
\renewcommand\thesection{\Roman{section}} % Roman numerals for the sections
\renewcommand\thesubsection{\roman{subsection}} % roman numerals for subsections
\titleformat{\section}[block]{\large\scshape\centering}{\thesection.}{1em}{} % Change the look of the section titles
\titleformat{\subsection}[block]{\large}{\thesubsection.}{1em}{} % Change the look of the section titles

\usepackage{fancyhdr} % Headers and footers
\pagestyle{fancy} % All pages have headers and footers
\fancyhead{} % Blank out the default header
\fancyfoot{} % Blank out the default footer
\fancyhead[C]{Reinforcement Learning  $\bullet$ September 2021 }% Custom header text
\fancyfoot[RO,LE]{\thepage} % Custom footer text

\usepackage{titling} % Customizing the title section

\usepackage{hyperref} % For hyperlinks in the PDF

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\setlength{\droptitle}{-4\baselineskip} % Move the title up

\pretitle{\begin{center}\Huge\bfseries} % Article title formatting
\posttitle{\end{center}} % Article title closing formatting
\title{Machine Learning: Reinforced Learning} % Article title
\author{%
\textsc{Robert Horton}\thanks{"Denim"} \\[1ex] % Your name
\normalsize University of Colorado Springs Colorado \\ % Your institution
\normalsize \href{mailto:rhorton2@uccs.edu}{rhorton2@uccs.edu} % Your email address
\and % Uncomment if 2 authors are required, duplicate these 4 lines if more
\textsc{Gaberial Yeager}\thanks{"Gabe"} \\[1ex] % Second author's name
\normalsize University of Colorado Springs Colorado \\ % Your institution
\normalsize \href{mailto:gyeager@uccs.edu}{gyeager@uccs.edu} % Second author's email address
}
\date{\today} % Leave empty to omit a date
\renewcommand{\maketitlehookd}{%
\begin{abstract}
 \noindent ABSTRACT HERE!!!!!

\end{abstract}
}

\renewcommand \thesection{\arabic{section}}
\renewcommand \thesubsection{\arabic{section}.\arabic{subsection}}
%----------------------------------------------------------------------------------------

\begin{document}

% Print the title
\maketitle

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section{Introduction}

 
\lettrine[nindent=0em,lines=3]{I}f you walk into any casino, you are likely to see one of the most popular card games in the world. Blackjack was invented in France about 300 years ago where it was known as “Vingt et Un” which means “twenty-one”. The objective is to get the sum of the value of your cards as close to 21 without going over 21. Going over 21 is know as a “break” which is a loss that gives the dealer a win. Its popularity is largely to the fact that is simple to learn. When playing blackjack there is a dealer and at least 1 player. Every card has a point value. Cards 2 through 10 are worth the corresponding number printed on them regardless of the color or suit. For example, 7 of hearts is simply worth 7 points. Face cards (jack, queen, king) are worth 10 points, and ace cards can be worth either 1 or 11 points, depending on whether or not the player “breaks” and goes over 21. A player is dealt two cards and may “hit” and receive another card or “stay” and refuse another card. There are a few cases that exist in casino blackjack that may be considered. In standard blackjack a player can split their hand up to 3 times, meaning the player can play up to 4 hands.  An ace and a card valued at 10 points beats all other cards, even if the sum of the other cards is 21. The player places a wager at the beginning of the hand and may change their bet after two cards are dealt. The dealer has a advantage over the players because of the hole card that players must guess to play against. This advantage of the dealer becomes 0.5\% if the player follows the basic blackjack strategy (Wong 1994). We will train an agent that will play blackjack according to policies it has learned during training. The agent will be trained with several different reinforced learning algorithms including Q-learning, Sarsa, and Temporal Difference will be compared to the optimal strategy and random plays (no strategy).

%------------------------------------------------

\section{Background}

Reinforcement learning, although considered a part of machine learning, is very different from the other types of machine learning.  Two main categories that the different types of machine learning fall under are supervised learning\footnote{Explanation of supervised learning} and unsupervised learning \footnote{Explanation of unsupervised learning} \cite{sutton:2018}.  Although supervised learning can be used to speed along the process of machine learning this is not always an option when deploying a machine learning process into an unexplored environment.  These types of implementations call upon reinforcement learning to perform an unsupervised learning process.  Now when performing these unsupervised approaches of machine learning there are many different methods that have been used and studied over the years.  Finding out which method is the most efficient and works best for training a machine to learn how to play blackjack will be the topic of this paper.\\ 

\indent When considering which method of RL\footnote{Reinforcement Learning} to use when wanting to teach a machine how to play the game blackjack, there are many different options to consider.  When considering the game blackjack the next cards that will be drawn are always unknown to the players and dealers.  The hole card\footnote{Face down card that is dealt to dealer at the beginning of each game} is also a card that is not known to any player including the dealer, making this game an even more inherently stochastic model.  Since bets are placed while playing this game, maximizing the profit on return for each game played is very important and is hard to do with the uncertainty of what the next card dealt will be and what hole card has been dealt to the dealer.  This models the perfect Markov Decision Process where each game promises states and actions that are unknown and have not been explored yet.\\

\indent When considering reinforcement learning it can be broken down further into two categories “On-Policy” or “Off-Policy” learning.  On-Policy RL approaches are considered as methods that involve learning agents that learn from the value function denoted as $v_{\pi}$ or $q_{\pi}$ according to future possible states and future accumaltive rewards from moving through these possible states.  These value functions for a given state at each step in time can be denoted as $v_{\pi}(s)$  or $q_{\pi}(s)$ for $s \in S$ where $S$ is the set of all possible states.   
\indent Off policy works when the learning agents learn by periodical taking actions that are not following the policy and rewards are not known to the agent.

of the previous occupied state \cite{GeeksforGeeks:2021dg}.  Above is a table that will be referred to when discussing what methods will be used for approaching these problems. \\ 

\begin{table}
\caption{Example table}
\centering
\begin{tabular}{llr}
\toprule
\multicolumn{2}{c}{Blackjack RL Methods} \\
\cmidrule(r){1-2}
Name & On/Off-Policy  & Greedy \\
\midrule
Q Learning & Off-Policy & ?? \\
SARSA & On-Policy & ?? \\
\bottomrule
\end{tabular}
\end{table}

%------------------------------------------------

\subsection{Q-Learning}

\indent One of the most well known and oldest methods listed is the “Q-Learning” approach.  This approach is considered an off-policy TD control algorithm that can be defined as\\

\begin{align*}
 Q(S_t,A_t) \leftarrow & Q(S_t,A_t)+ \alpha\big[ R_{t+1} +\\
& \gamma \binom{max}{a} Q(S_{t+1}, a) - Q(S_t,A_t)\big]
\end{align*}

[1]\\
, where the steps taken are independent of what the next updated policy might be based upon that action.  Depending upon the value of $\epsilon$ the frequency of random steps taken can vary and vary in how stochastic the model is.  What is important to note is that even though these models are stochastic the models are able quickly able to ensure a steady and step increase in profit after each game is played. Since this methods relies on deciding what actions to take based off of recent actions, the decision is completely independent of future rewards that might be gained by moving into the decided state, other then the current policy being used. 

%------------------------------------------------

\subsection{SARSA}

\indent The “SARSA” \footnote{State Action Reward State Action} is a slightly modified version of the Q -learning approach where the modification of the policy differs. Q-learning is considered an off policy where SARSA is an on policy \cite{GeeksforGeeks:2021dg}.  The difference between the two, as iterated before, is that the action that is chosen to be taken is based off the accumulative reward off all future states that could possibly betaken.  This approach does take a slightly more greedy approach but still involves exploration based off a given learning rate specified with in the model parameters. 

%------------------------------------------------

\section{Related Work}

%------------------------------------------------

\section{Methodology}

A statement requiring citation \cite{Figueredo:2009dg}.

%------------------------------------------------

\section{Conclusion}

\pagebreak
%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------

\bibliography{Reinforcement Learning Proposal Rough Draft.bib}

\begin{thebibliography}{99} % Bibliography - this is intentionally simple in this template
\bibitem[Sutton, R. S., \& Barto, A. G. , 2018]{sutton:2018}
Sutton, R. S., \& Barto, A. G. (2018).
\newblock Reinforcement Learning: An Introduction. Cambridge (Mass.): 
\newblock {\em The MIT Press.}, .

\bibitem[GeeksforGeeks]{GeeksforGeeks:2021dg}
“Sarsa Reinforcement Learning.” {\em GeeksforGeeks}, \\
24 June 2021,\\
\newblock www.geeksforgeeks.org/sarsa-reinforcement-learning/. 


\bibitem[Figueredo and Wolf, 2009]{Figueredo:2009dg}
Figueredo, A.~J. and Wolf, P. S.~A. (2009).
\newblock Assortative pairing and life history strategy - a cross-cultural
  study.
\newblock {\em Human Nature}, 20:317--330.
\end{thebibliography}

%----------------------------------------------------------------------------------------

\end{document}
